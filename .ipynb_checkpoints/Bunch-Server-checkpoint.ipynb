{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from zipfile import ZipFile\n",
    "from filecmp import dircmp\n",
    "\n",
    "\n",
    "def alvin_god_bunch(user_n_cluster, dir_arr, file_version, main_dict, bunch_filename, bunch_type):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(dir_arr)\n",
    "\n",
    "    true_k = user_n_cluster\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    model.fit(X)\n",
    "\n",
    "    #print(\"Top terms per cluster:\")\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    main_cluster_arr = []\n",
    "    col_counter = 0\n",
    "\n",
    "    \n",
    "    #cluster = AgglomerativeClustering(n_clusters=user_n_cluster, affinity=user_affinity, linkage=user_linkage)\n",
    "    #cluster_result = cluster.fit_predict(depends_arr)\n",
    "\n",
    "    #cluster_result_arr = []\n",
    "    #cluster_counter = 0\n",
    "    #for element in cluster_result:\n",
    "        #print(element)\n",
    "        #cluster_result_arr.append(element)\n",
    "\n",
    "    #main_cluster_arr.append(cluster_result_arr)\n",
    "    \n",
    "    col_counter += 1\n",
    "    \n",
    "    dir_arr_int = []\n",
    "    #for element in dir_arr:\n",
    "    #    dir_arr_int.append(main_dict[element])\n",
    "    \n",
    "    #cluster_result_int = []\n",
    "    #for element in cluster_result:\n",
    "        #cluster_result_int.append(main_dict[main_dict_val[element]])\n",
    "    \n",
    "    #print(dir_arr_int)\n",
    "    #print('fk')\n",
    "    #print(cluster_result_int)\n",
    "    #common_results_rsf = common_member(dir_arr_int,cluster_result_int)\n",
    "    #print(len(common_results_rsf))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    filename2 = 'MoJo_1.2.1/' + project_name + '/' +str(file_version) + '_' + bunch_type + '_a' +\".rsf\"\n",
    "    bunch_counter = 0\n",
    "    max_element = 0\n",
    "    arr_unique = []\n",
    "    with open(filename2, 'w') as f:\n",
    "        bunch_file = open(bunch_filename)\n",
    "        table = []\n",
    "        for line in bunch_file:\n",
    "            bunch_counter += 1\n",
    "            line = line.replace('\\n', '')\n",
    "            line = line.split(' = ')\n",
    "            line[0] = line[0].replace('SS(', '')\n",
    "            line[0] = line[0].replace(')', '')\n",
    "            line[1] = line[1].split(sep = ', ')\n",
    "            table.append(line)\n",
    "        \n",
    "        for i in range(len(table)):\n",
    "            for j in range(len(table[i][1])):\n",
    "                string = \"contain \" + str(table[i][0]) + \" \" + str(table[i][1][j]) + \"\\n\"\n",
    "                arr_unique.append(int(table[i][1][j]))\n",
    "                if int(table[i][1][j]) > max_element:\n",
    "                    max_element = int(table[i][1][j])\n",
    "                f.write(string)\n",
    "        bunch_file.close()\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    filename1 = 'MoJo_1.2.1/' + project_name + '/' +str(file_version) + '_' + bunch_type +'_b' +\".rsf\"\n",
    "    #print(filename1)\n",
    "    with open(filename1, 'w') as f:\n",
    "        for i in range(len(dir_arr)):\n",
    "            Y = vectorizer.transform([dir_arr[i]])\n",
    "            if int(i) in arr_unique:\n",
    "                string = \"contain \" + str(model.predict(Y)[0]) + \" \" +str(i) + \"\\n\"\n",
    "     \n",
    "                f.write(string)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "        #for element in cluster_result:\n",
    "            #print(element)\n",
    "            #string = \"contain \" + str(element) + \" \" + str(main_dict[main_dict_val[element]]) + \"\\n\"\n",
    "            #f.write(string)\n",
    "        #f.close()\n",
    "\n",
    "    #print('Difference is: ' + str(len(dir_arr) - len(G.nodes) ))\n",
    "    if len(dir_arr) - max_element > 0:\n",
    "        with open(filename2,'a') as f:\n",
    "            for i in range(len(dir_arr)-max_element):\n",
    "                tbc = (i+user_n_cluster)\n",
    "                string = \"contain \" + str(tbc) + \" \" + str(tbc) + \"\\n\"\n",
    "                f.write(string)\n",
    "\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(filename1,'a') as f:\n",
    "            for i in range(max_element - len(dir_arr) + 1):\n",
    "                tbc = i+len(dir_arr)\n",
    "                if int(tbc) in arr_unique:\n",
    "                    string = \"contain \" + str(tbc) + \" \" + str(tbc) + \"\\n\"\n",
    "                    f.write(string)\n",
    "            f.close()\n",
    "\n",
    "    return filename1, filename2\n",
    "\n",
    "def fileIsSame(right, left, path):\n",
    "    return os.path.exists (os.path.join(left, path.replace(right, '')));\n",
    "\n",
    "def compare(right, left):\n",
    "    difference = list();\n",
    "    for root, dirs, files in os.walk(right):\n",
    "        for name in files:\n",
    "            path = os.path.join(root, name);\n",
    "            # check if file is same\n",
    "            if fileIsSame(right, left, path):\n",
    "                if os.path.isdir(path):\n",
    "                    # recursively check subdirs\n",
    "                    difference.extend(compare(path, left));\n",
    "            else:\n",
    "                # count file as difference\n",
    "                difference.append(path);\n",
    "\n",
    "    return difference;\n",
    "\n",
    "def compare_similar(right, left):\n",
    "    difference = list();\n",
    "    for root, dirs, files in os.walk(right):\n",
    "        for name in files:\n",
    "            path = os.path.join(root, name);\n",
    "            # check if file is same\n",
    "            if fileIsSame(right, left, path):\n",
    "                \n",
    "                if os.path.isdir(path):\n",
    "                    # recursively check subdirs\n",
    "                    difference.extend(compare_similar(path, left));\n",
    "                difference.append(path)\n",
    "                \n",
    "            #else:\n",
    "                # count file as difference\n",
    "                #difference.append(path);\n",
    "\n",
    "    return difference;\n",
    "\n",
    "\n",
    "\n",
    "def common_member(a,b):\n",
    "    a_set = set(a)\n",
    "    b_set = set(b)\n",
    "    if len(a_set.intersection(b_set)) > 0:\n",
    "        return(a_set.intersection(b_set))\n",
    "    else:\n",
    "        return set()\n",
    "    \n",
    "\n",
    "project_names = ['apache-storm',\n",
    "'apache-cassandra',\n",
    "'airbnb-lottie-android',\n",
    "'apache-isis',\n",
    "'apache-jmeter',\n",
    "'apache-log4j',\n",
    "'apache-maven',\n",
    "'apache-spark',\n",
    "'apache-tomcat',\n",
    "'rzwitserloot-lombok',\n",
    "'apache-tika',\n",
    "'alibaba-fastjson',\n",
    "'activiti-activiti',\n",
    "'bumptech-glide',\n",
    "'codecentric-spring-boot-admin',\n",
    "'dropwizard-dropwizard',\n",
    "'dropwizard-metrics',\n",
    "'facebook-facebook-android-sdk',\n",
    "'google-dagger',\n",
    "'google-error-prone',\n",
    "'grpc-grpc-java',\n",
    "'java-native-accessjna',\n",
    "'jenkinsci-jenkins',\n",
    "'jhy-jsoup',\n",
    "'mockito-mockito',\n",
    "'mybatis-mybatis-3',\n",
    "'naver-pinpoint',\n",
    "'pxb1988-dex2jar',\n",
    "'ReactiveX-RxJava',\n",
    "'redisson-redisson',\n",
    "'swagger-api-swagger-core']\n",
    "\n",
    "for project_name in project_names:\n",
    "    try:\n",
    "        ver = pd.read_csv(project_name + '.csv', encoding='ISO-8859-1')\n",
    "        ver.head()\n",
    "\n",
    "        release_arr = []\n",
    "\n",
    "        link = open(project_name + '.txt')\n",
    "        for line in link:\n",
    "            link_str = line\n",
    "\n",
    "        link.close()\n",
    "\n",
    "\n",
    "        os.mkdir('C:/Users/user/Desktop/FIT4003/raw_sourcecode/' + project_name)\n",
    "\n",
    "        for index, row in ver.iterrows():\n",
    "            #print(index)\n",
    "            #print('lol')\n",
    "            #print(row['release_tag'])\n",
    "            release_arr.append((row['release_tag'], row['version_name']))\n",
    "\n",
    "        for element in release_arr:\n",
    "            command = 'cd C:/Users/user/Desktop/FIT4003/raw_sourcecode/' + project_name + ' & mkdir ' + project_name + '-' + element[1]\n",
    "            print(command)\n",
    "            os.system(command)\n",
    "\n",
    "            command = 'git clone ' + link_str +  ' C:/Users/user/Desktop/FIT4003/raw_sourcecode/' + project_name + '/' + project_name + '-' +element[1]\n",
    "            print(command)\n",
    "            os.system(command)\n",
    "\n",
    "            command = 'cd C:/Users/user/Desktop/FIT4003/raw_sourcecode/' + project_name + '/' + project_name +'-' + element[1] + ' & git checkout ' + element[0]\n",
    "            print(command)\n",
    "            os.system(command)\n",
    "\n",
    "\n",
    "        rootdir = 'raw_sourcecode'\n",
    "        dir_arr = []\n",
    "        rootdir_arr = []\n",
    "        depth=2\n",
    "\n",
    "        for root, dirs, files in os.walk(rootdir):\n",
    "            if root.count(os.sep) == depth and project_name in str(root):\n",
    "                #print(root)\n",
    "                dir_arr.append(root)\n",
    "                rootdir_arr.append(root)\n",
    "                #for file in files: \n",
    "                    #print(os.path.join(subdir, file))\n",
    "                ### Only look for Java Files ###\n",
    "                #if \"Apache Spark versions\" in str(os.path.join(subdir, file)):\n",
    "                    #print(os.path.join(subdir, file))\n",
    "\n",
    "                    #dir_arr.append(str(os.path.join(subdir, file)))\n",
    "\n",
    "        #print(len(dir_arr))\n",
    "\n",
    "\n",
    "        os.mkdir('MoJo_1.2.1/' + project_name)\n",
    "        os.mkdir('raw_depends/' + project_name)\n",
    "        for i in range(len(dir_arr)):\n",
    "            #print(element)\n",
    "            command = 'cd C:/Users/user/Desktop/FIT4003/depends-0.9.2 & ' + 'java -jar depends.jar java C:/Users/user/Desktop/FIT4003/raw_sourcecode/'  + project_name + '/' + dir_arr[i].split('\\\\')[-1] +  ' ../raw_depends/' + project_name + '/' + str(i) + '_' + dir_arr[i].split('\\\\')[-1] \n",
    "            #print(command)\n",
    "            #os.system('cd C:/Users/user/Desktop/FIT4003/MoJo_1.2.1 & java MoJo test/test2_a.rsf test/test2_b.rsf > test/test_results.txt')\n",
    "            os.system(command)\n",
    "\n",
    "        rootdir = 'raw_depends/' + project_name\n",
    "        json_name_dict = {}\n",
    "        json_result_dict = {}\n",
    "        for root, dirs, files in os.walk(rootdir):\n",
    "\n",
    "            #print(files)\n",
    "            for element in files:\n",
    "\n",
    "                with open(rootdir + '/' + element) as f:\n",
    "                    tmp = json.load(f)\n",
    "\n",
    "                element = element.split('_')\n",
    "                json_name_dict[element[0]] = element[1]\n",
    "                json_result_dict[element[0]] = tmp\n",
    "                \n",
    "        os.mkdir('raw_mdg_results/' + project_name)\n",
    "        os.mkdir('raw_mdg/' + project_name)\n",
    "\n",
    "        ### Getting the ground truth by comparing previous 10 versions (Burden AF)\n",
    "        ground_truth_dict = {}\n",
    "        max_len_tmp = 0\n",
    "        root_dir = 'raw_sourcecode/' + project_name + '/'\n",
    "        for i in range(10,20):\n",
    "            file1 = str(root_dir +json_name_dict[str(i)]).replace('.json', '') + '/'\n",
    "            for j in range(1,11):\n",
    "                file2 = str(root_dir + json_name_dict[str(i-j)]).replace('.json', '') + '/'\n",
    "\n",
    "                #test = compare_similar()\n",
    "                #print(file1, file2)\n",
    "                if (j == 1):\n",
    "                    prev_tmp = compare_similar(file1, file2)\n",
    "                else:\n",
    "                    tmp = compare_similar(file1, file2)\n",
    "                    tmp = common_member(prev_tmp, tmp)\n",
    "                    #print(len(tmp))\n",
    "                    prev_tmp = tmp\n",
    "\n",
    "            if len(prev_tmp) > max_len_tmp:\n",
    "                max_len_tmp = len(prev_tmp)\n",
    "            #print(len(prev_tmp))\n",
    "            ground_truth_dict[str(i)] = prev_tmp\n",
    "\n",
    "\n",
    "        for i in range(len(ground_truth_dict)):\n",
    "            #print(i)\n",
    "            dir_arr = []\n",
    "            for element in ground_truth_dict[str(i+10)]:\n",
    "                if \".java\" in element:\n",
    "                    element = element.replace('\\\\', '/')\n",
    "                    element = element.replace('raw_sourcecode/' + project_name + '/' + json_name_dict[str(i+10)].split('.json')[0] + '/', '' )\n",
    "                    element = element.replace('/', '\\\\')\n",
    "                    element = '\\\\' + element\n",
    "                    dir_arr.append(element)\n",
    "\n",
    "            #print('raw_depends/apache_log4j/' + str(i+10)+ '_'+ json_name_dict[str(i+10)])\n",
    "            with open('raw_depends/' + project_name + '/' + str(i+10)+ '_'+ json_name_dict[str(i+10)]) as f:\n",
    "                spark_results = json.load(f)\n",
    "\n",
    "            main_dict = {}\n",
    "            main_dict_val = {}\n",
    "            main_dict_counter = 0\n",
    "            dict_array = []\n",
    "            min_set = set()\n",
    "            for element in spark_results['variables']:\n",
    "                #print(element)\n",
    "\n",
    "                ### Replace this with the initial JSON directory ###\n",
    "                #print(json_name_dict[str(i+10)])\n",
    "                element_tbc = element.replace('C:\\\\Users\\\\user\\\\Desktop\\\\FIT4003\\\\raw_sourcecode\\\\' + project_name + '\\\\' +json_name_dict[str(i+10)].split('.json')[0] , '')\n",
    "                #print(element)\n",
    "                main_dict[element_tbc] = main_dict_counter\n",
    "                main_dict_val[main_dict_counter] = element_tbc\n",
    "                main_dict_counter += 1\n",
    "\n",
    "\n",
    "            index  = 0\n",
    "            var_array = []\n",
    "            for value in spark_results['variables']:\n",
    "                var_array.append([index, value])\n",
    "                #print(index, value)\n",
    "                index += 1\n",
    "\n",
    "            var_df = pd.DataFrame(var_array)\n",
    "            var_df.columns = ['index_val', 'name']\n",
    "\n",
    "            feature_list = {}\n",
    "            feature_index = 2\n",
    "            for element in spark_results['cells']:\n",
    "                #print(element)\n",
    "                try:\n",
    "                    for a in element['values']:\n",
    "                        if a not in feature_list:\n",
    "                            feature_list[a] = feature_index\n",
    "                            feature_index += 1\n",
    "                        #print(a['Call'])\n",
    "\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            feature_arr = []\n",
    "            for element in spark_results['cells']:\n",
    "                #print(array)\n",
    "                array = [0] * (len(feature_list) + 2)\n",
    "                values = dict(element['values'])\n",
    "                #print(element)\n",
    "                array[0] = element['src']\n",
    "                array[1] = element['dest']\n",
    "                for feature in feature_list:\n",
    "                    try:\n",
    "                        value = values[feature]\n",
    "                        array[feature_list[feature]] = value\n",
    "                    except:\n",
    "                        pass\n",
    "                #print(array)\n",
    "                feature_arr.append(array)\n",
    "\n",
    "            feature_df = pd.DataFrame(feature_arr)\n",
    "            col_names = ['src', 'dest']\n",
    "            for element in feature_list:\n",
    "                col_names.append(element)\n",
    "            feature_df.columns = col_names\n",
    "            test = feature_df[['src', 'dest']]\n",
    "            tmp_mdg_filename = 'raw_mdg/' + project_name + '/' + json_name_dict[str(i+10)].split('.json')[0] + '.mdg'\n",
    "            tmp_mdg = open(tmp_mdg_filename, 'w+')\n",
    "            for index, row in test.iterrows():\n",
    "                #print(row['src'],row['dest'])\n",
    "                tmp_mdg.write(str(row['src']) + ' ' + str(row['dest']) + '\\n')\n",
    "\n",
    "            tmp_mdg.close()\n",
    "\n",
    "            bunch_result_filename = 'raw_mdg_results/' + project_name + '/' + json_name_dict[str(i+10)].split('.json')[0]\n",
    "\n",
    "            hillclimbing_filename = bunch_result_filename + '-hillclimbing'\n",
    "            ga_filename = bunch_result_filename + '-ga'\n",
    "            exhaustive_filename = bunch_result_filename + '-exhaustive'\n",
    "\n",
    "            command = \"java -jar alvin_bunch_automation.jar \" + tmp_mdg_filename + \" \" + hillclimbing_filename +  \" hillclimbing\"\n",
    "            print(command)\n",
    "            os.system(command)\n",
    "\n",
    "            command = \"java -jar alvin_bunch_automation.jar \" + tmp_mdg_filename + \" \" + ga_filename +  \" ga\"\n",
    "            print(command)\n",
    "            os.system(command)\n",
    "\n",
    "            command = \"java -jar alvin_bunch_automation.jar \" + tmp_mdg_filename + \" \" + exhaustive_filename +  \" exhaustive\"\n",
    "            print(command)\n",
    "            os.system(command)\n",
    "\n",
    "            filename_arr = []\n",
    "\n",
    "            number = 0\n",
    "            with open(hillclimbing_filename + '.bunch') as f:\n",
    "                for line in f:\n",
    "                    number += 1\n",
    "            try:\n",
    "                filename_1, filename_2 = alvin_god_bunch(number, dir_arr, json_name_dict[str(i+10)].split('.json')[0], main_dict,hillclimbing_filename+ '.bunch', 'hillclimbing')\n",
    "                filename_1 = filename_1.replace('MoJo_1.2.1/', '')\n",
    "                filename_2 = filename_2.replace('MoJo_1.2.1/', '')\n",
    "                filename_arr.append((filename_1, filename_2))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "            number = 0\n",
    "            with open(ga_filename+ '.bunch') as f:\n",
    "                for line in f:\n",
    "                    number += 1\n",
    "\n",
    "            try:\n",
    "                filename_1, filename_2 = alvin_god_bunch(number, dir_arr, json_name_dict[str(i+10)].split('.json')[0], main_dict,ga_filename+ '.bunch', 'ga')\n",
    "                filename_1 = filename_1.replace('MoJo_1.2.1/', '')\n",
    "                filename_2 = filename_2.replace('MoJo_1.2.1/', '')\n",
    "                filename_arr.append((filename_1, filename_2))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "            number = 0\n",
    "            with open(exhaustive_filename+ '.bunch') as f:\n",
    "                for line in f:\n",
    "                    number += 1\n",
    "\n",
    "            try:\n",
    "                filename_1, filename_2 = alvin_god_bunch(number, dir_arr, json_name_dict[str(i+10)].split('.json')[0], main_dict,exhaustive_filename+ '.bunch', 'exhaustive')\n",
    "                filename_1 = filename_1.replace('MoJo_1.2.1/', '')\n",
    "                filename_2 = filename_2.replace('MoJo_1.2.1/', '')\n",
    "                filename_arr.append((filename_1, filename_2))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "\n",
    "            for i in range(len(filename_arr)):\n",
    "\n",
    "                command = 'cd C:/Users/user/Desktop/FIT4003/MoJo_1.2.1 & ' + 'java MoJo ' + filename_arr[i][1] + ' ' + filename_arr[i][0] + ' >> ' + project_name + '/' + project_name + '_results_bunch.txt'\n",
    "                #print(command)\n",
    "                #os.system('cd C:/Users/user/Desktop/FIT4003/MoJo_1.2.1 & java MoJo test/test2_a.rsf test/test2_b.rsf > test/test_results.txt')\n",
    "                os.system(command)\n",
    "                \n",
    "    except Exception as e:\n",
    "        \n",
    "        print(project_name + ' Failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-2.7.mdg raw_mdg_results/apache-jmeter/apache-jmeter-2.7-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-2.7.mdg raw_mdg_results/apache-jmeter/apache-jmeter-2.7-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-2.7.mdg raw_mdg_results/apache-jmeter/apache-jmeter-2.7-exhaustive exhaustive\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-2.8.mdg raw_mdg_results/apache-jmeter/apache-jmeter-2.8-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-2.8.mdg raw_mdg_results/apache-jmeter/apache-jmeter-2.8-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-2.8.mdg raw_mdg_results/apache-jmeter/apache-jmeter-2.8-exhaustive exhaustive\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-2.9.mdg raw_mdg_results/apache-jmeter/apache-jmeter-2.9-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-2.9.mdg raw_mdg_results/apache-jmeter/apache-jmeter-2.9-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-2.9.mdg raw_mdg_results/apache-jmeter/apache-jmeter-2.9-exhaustive exhaustive\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.0.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.0-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.0.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.0-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.0.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.0-exhaustive exhaustive\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.1.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.1-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.1.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.1-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.1.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.1-exhaustive exhaustive\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.2.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.2-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.2.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.2-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.2.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.2-exhaustive exhaustive\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.3.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.3-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.3.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.3-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-3.3.mdg raw_mdg_results/apache-jmeter/apache-jmeter-3.3-exhaustive exhaustive\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-4.0.mdg raw_mdg_results/apache-jmeter/apache-jmeter-4.0-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-4.0.mdg raw_mdg_results/apache-jmeter/apache-jmeter-4.0-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-4.0.mdg raw_mdg_results/apache-jmeter/apache-jmeter-4.0-exhaustive exhaustive\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-5.0.mdg raw_mdg_results/apache-jmeter/apache-jmeter-5.0-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-5.0.mdg raw_mdg_results/apache-jmeter/apache-jmeter-5.0-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-5.0.mdg raw_mdg_results/apache-jmeter/apache-jmeter-5.0-exhaustive exhaustive\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-5.1.mdg raw_mdg_results/apache-jmeter/apache-jmeter-5.1-hillclimbing hillclimbing\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-5.1.mdg raw_mdg_results/apache-jmeter/apache-jmeter-5.1-ga ga\n",
      "java -jar alvin_bunch_automation.jar raw_mdg/apache-jmeter/apache-jmeter-5.1.mdg raw_mdg_results/apache-jmeter/apache-jmeter-5.1-exhaustive exhaustive\n"
     ]
    }
   ],
   "source": [
    "os.mkdir('raw_mdg_results/' + project_name)\n",
    "os.mkdir('raw_mdg/' + project_name)\n",
    "\n",
    "### Getting the ground truth by comparing previous 10 versions (Burden AF)\n",
    "ground_truth_dict = {}\n",
    "max_len_tmp = 0\n",
    "root_dir = 'raw_sourcecode/' + project_name + '/'\n",
    "for i in range(10,20):\n",
    "    file1 = str(root_dir +json_name_dict[str(i)]).replace('.json', '') + '/'\n",
    "    for j in range(1,11):\n",
    "        file2 = str(root_dir + json_name_dict[str(i-j)]).replace('.json', '') + '/'\n",
    "        \n",
    "        #test = compare_similar()\n",
    "        #print(file1, file2)\n",
    "        if (j == 1):\n",
    "            prev_tmp = compare_similar(file1, file2)\n",
    "        else:\n",
    "            tmp = compare_similar(file1, file2)\n",
    "            tmp = common_member(prev_tmp, tmp)\n",
    "            #print(len(tmp))\n",
    "            prev_tmp = tmp\n",
    "            \n",
    "    if len(prev_tmp) > max_len_tmp:\n",
    "        max_len_tmp = len(prev_tmp)\n",
    "    #print(len(prev_tmp))\n",
    "    ground_truth_dict[str(i)] = prev_tmp\n",
    "    \n",
    "    \n",
    "for i in range(len(ground_truth_dict)):\n",
    "    #print(i)\n",
    "    dir_arr = []\n",
    "    for element in ground_truth_dict[str(i+10)]:\n",
    "        if \".java\" in element:\n",
    "            element = element.replace('\\\\', '/')\n",
    "            element = element.replace('raw_sourcecode/' + project_name + '/' + json_name_dict[str(i+10)].split('.json')[0] + '/', '' )\n",
    "            element = element.replace('/', '\\\\')\n",
    "            element = '\\\\' + element\n",
    "            dir_arr.append(element)\n",
    "            \n",
    "    #print('raw_depends/apache_log4j/' + str(i+10)+ '_'+ json_name_dict[str(i+10)])\n",
    "    with open('raw_depends/' + project_name + '/' + str(i+10)+ '_'+ json_name_dict[str(i+10)]) as f:\n",
    "        spark_results = json.load(f)\n",
    "        \n",
    "    main_dict = {}\n",
    "    main_dict_val = {}\n",
    "    main_dict_counter = 0\n",
    "    dict_array = []\n",
    "    min_set = set()\n",
    "    for element in spark_results['variables']:\n",
    "        #print(element)\n",
    "\n",
    "        ### Replace this with the initial JSON directory ###\n",
    "        #print(json_name_dict[str(i+10)])\n",
    "        element_tbc = element.replace('C:\\\\Users\\\\tanji\\\\Desktop\\\\FIT4003\\\\raw_sourcecode\\\\' + project_name + '\\\\' +json_name_dict[str(i+10)].split('.json')[0] , '')\n",
    "        #print(element)\n",
    "        main_dict[element_tbc] = main_dict_counter\n",
    "        main_dict_val[main_dict_counter] = element_tbc\n",
    "        main_dict_counter += 1\n",
    " \n",
    "    \n",
    "    index  = 0\n",
    "    var_array = []\n",
    "    for value in spark_results['variables']:\n",
    "        var_array.append([index, value])\n",
    "        #print(index, value)\n",
    "        index += 1\n",
    "    \n",
    "    var_df = pd.DataFrame(var_array)\n",
    "    var_df.columns = ['index_val', 'name']\n",
    "    \n",
    "    feature_list = {}\n",
    "    feature_index = 2\n",
    "    for element in spark_results['cells']:\n",
    "        #print(element)\n",
    "        try:\n",
    "            for a in element['values']:\n",
    "                if a not in feature_list:\n",
    "                    feature_list[a] = feature_index\n",
    "                    feature_index += 1\n",
    "                #print(a['Call'])\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    feature_arr = []\n",
    "    for element in spark_results['cells']:\n",
    "        #print(array)\n",
    "        array = [0] * (len(feature_list) + 2)\n",
    "        values = dict(element['values'])\n",
    "        #print(element)\n",
    "        array[0] = element['src']\n",
    "        array[1] = element['dest']\n",
    "        for feature in feature_list:\n",
    "            try:\n",
    "                value = values[feature]\n",
    "                array[feature_list[feature]] = value\n",
    "            except:\n",
    "                pass\n",
    "        #print(array)\n",
    "        feature_arr.append(array)\n",
    "        \n",
    "    feature_df = pd.DataFrame(feature_arr)\n",
    "    col_names = ['src', 'dest']\n",
    "    for element in feature_list:\n",
    "        col_names.append(element)\n",
    "    feature_df.columns = col_names\n",
    "    test = feature_df[['src', 'dest']]\n",
    "    tmp_mdg_filename = 'raw_mdg/' + project_name + '/' + json_name_dict[str(i+10)].split('.json')[0] + '.mdg'\n",
    "    tmp_mdg = open(tmp_mdg_filename, 'w+')\n",
    "    for index, row in test.iterrows():\n",
    "        #print(row['src'],row['dest'])\n",
    "        tmp_mdg.write(str(row['src']) + ' ' + str(row['dest']) + '\\n')\n",
    "\n",
    "    tmp_mdg.close()\n",
    "    \n",
    "    bunch_result_filename = 'raw_mdg_results/' + project_name + '/' + json_name_dict[str(i+10)].split('.json')[0]\n",
    "    \n",
    "    hillclimbing_filename = bunch_result_filename + '-hillclimbing'\n",
    "    ga_filename = bunch_result_filename + '-ga'\n",
    "    exhaustive_filename = bunch_result_filename + '-exhaustive'\n",
    "    \n",
    "    command = \"java -jar alvin_bunch_automation.jar \" + tmp_mdg_filename + \" \" + hillclimbing_filename +  \" hillclimbing\"\n",
    "    print(command)\n",
    "    os.system(command)\n",
    "    \n",
    "    command = \"java -jar alvin_bunch_automation.jar \" + tmp_mdg_filename + \" \" + ga_filename +  \" ga\"\n",
    "    print(command)\n",
    "    os.system(command)\n",
    "    \n",
    "    command = \"java -jar alvin_bunch_automation.jar \" + tmp_mdg_filename + \" \" + exhaustive_filename +  \" exhaustive\"\n",
    "    print(command)\n",
    "    os.system(command)\n",
    "    \n",
    "    filename_arr = []\n",
    "    \n",
    "    number = 0\n",
    "    with open(hillclimbing_filename + '.bunch') as f:\n",
    "        for line in f:\n",
    "            number += 1\n",
    "    try:\n",
    "        filename_1, filename_2 = alvin_god_bunch(number, dir_arr, json_name_dict[str(i+10)].split('.json')[0], main_dict,hillclimbing_filename+ '.bunch', 'hillclimbing')\n",
    "        filename_1 = filename_1.replace('MoJo_1.2.1/', '')\n",
    "        filename_2 = filename_2.replace('MoJo_1.2.1/', '')\n",
    "        filename_arr.append((filename_1, filename_2))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    number = 0\n",
    "    with open(ga_filename+ '.bunch') as f:\n",
    "        for line in f:\n",
    "            number += 1\n",
    "    \n",
    "    try:\n",
    "        filename_1, filename_2 = alvin_god_bunch(number, dir_arr, json_name_dict[str(i+10)].split('.json')[0], main_dict,ga_filename+ '.bunch', 'ga')\n",
    "        filename_1 = filename_1.replace('MoJo_1.2.1/', '')\n",
    "        filename_2 = filename_2.replace('MoJo_1.2.1/', '')\n",
    "        filename_arr.append((filename_1, filename_2))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    number = 0\n",
    "    with open(exhaustive_filename+ '.bunch') as f:\n",
    "        for line in f:\n",
    "            number += 1\n",
    "    \n",
    "    try:\n",
    "        filename_1, filename_2 = alvin_god_bunch(number, dir_arr, json_name_dict[str(i+10)].split('.json')[0], main_dict,exhaustive_filename+ '.bunch', 'exhaustive')\n",
    "        filename_1 = filename_1.replace('MoJo_1.2.1/', '')\n",
    "        filename_2 = filename_2.replace('MoJo_1.2.1/', '')\n",
    "        filename_arr.append((filename_1, filename_2))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    \n",
    "    for i in range(len(filename_arr)):\n",
    "    \n",
    "        command = 'cd C:/Users/tanji/Desktop/FIT4003/MoJo_1.2.1 & ' + 'java MoJo ' + filename_arr[i][1] + ' ' + filename_arr[i][0] + ' >> ' + project_name + '/' + project_name + '_results_bunch.txt'\n",
    "        #print(command)\n",
    "        #os.system('cd C:/Users/tanji/Desktop/FIT4003/MoJo_1.2.1 & java MoJo test/test2_a.rsf test/test2_b.rsf > test/test_results.txt')\n",
    "        os.system(command)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Final_Results/' + project_name + '/num_class.txt', 'w')\n",
    "f.write(str(max_len_tmp))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'apache-log4j-1.0.json',\n",
       " '10': 'apache-log4j-1.2.13.json',\n",
       " '11': 'apache-log4j-1.2.14.json',\n",
       " '12': 'apache-log4j-1.2.15.json',\n",
       " '13': 'apache-log4j-1.2.16.json',\n",
       " '14': 'apache-log4j-1.2.17.json',\n",
       " '15': 'apache-log4j-1.2.2.json',\n",
       " '16': 'apache-log4j-1.2.3.json',\n",
       " '17': 'apache-log4j-1.2.4.json',\n",
       " '18': 'apache-log4j-1.2.6.json',\n",
       " '19': 'apache-log4j-1.2.7.json',\n",
       " '1': 'apache-log4j-1.0.1.json',\n",
       " '20': 'apache-log4j-1.2.9.json',\n",
       " '2': 'apache-log4j-1.0.4.json',\n",
       " '3': 'apache-log4j-1.1.json',\n",
       " '4': 'apache-log4j-1.1.1.json',\n",
       " '5': 'apache-log4j-1.1.2.json',\n",
       " '6': 'apache-log4j-1.1.3.json',\n",
       " '7': 'apache-log4j-1.2.1.json',\n",
       " '8': 'apache-log4j-1.2.11.json',\n",
       " '9': 'apache-log4j-1.2.12.json'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dest</th>\n",
       "      <th>Call</th>\n",
       "      <th>Extend</th>\n",
       "      <th>Use</th>\n",
       "      <th>Import</th>\n",
       "      <th>Create</th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Contain</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Return</th>\n",
       "      <th>Implement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>121</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>121</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>121</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   src  dest  Call  Extend   Use  Import  Create  Parameter  Contain  Cast  \\\n",
       "0   24    18   2.0     1.0  13.0     0.0     0.0        0.0      0.0   0.0   \n",
       "1  121    28   0.0     0.0   0.0     1.0     0.0        0.0      0.0   0.0   \n",
       "2    5     4   1.0     0.0   0.0     0.0     1.0        0.0      0.0   0.0   \n",
       "3  121    29   0.0     0.0   0.0     1.0     0.0        0.0      0.0   0.0   \n",
       "4  121    26   0.0     0.0   0.0     1.0     0.0        0.0      0.0   0.0   \n",
       "5    7    98   0.0     0.0   0.0     1.0     0.0        1.0      0.0   0.0   \n",
       "6   57    30   1.0     0.0   0.0     0.0     0.0        0.0      0.0   0.0   \n",
       "7  121    27   0.0     0.0   0.0     1.0     0.0        0.0      0.0   0.0   \n",
       "8  121    24   0.0     0.0   0.0     1.0     0.0        0.0      0.0   0.0   \n",
       "9  121    25   0.0     0.0   0.0     1.0     0.0        0.0      0.0   0.0   \n",
       "\n",
       "   Return  Implement  \n",
       "0     0.0        0.0  \n",
       "1     0.0        0.0  \n",
       "2     0.0        0.0  \n",
       "3     0.0        0.0  \n",
       "4     0.0        0.0  \n",
       "5     0.0        0.0  \n",
       "6     0.0        0.0  \n",
       "7     0.0        0.0  \n",
       "8     0.0        0.0  \n",
       "9     0.0        0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = feature_df[['src', 'dest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   src  dest\n",
       "0   24    18\n",
       "1  121    28\n",
       "2    5     4\n",
       "3  121    29\n",
       "4  121    26"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_mdg_filename = 'apache-log4j-test.mdg'\n",
    "tmp_mdg = open(tmp_mdg_filename, 'w+')\n",
    "for index, row in test.iterrows():\n",
    "    #print(row['src'],row['dest'])\n",
    "    tmp_mdg.write(str(row['src']) + ' ' + str(row['dest']) + '\\n')\n",
    "    \n",
    "tmp_mdg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -jar alvin_bunch_automation.jar apache-log4j-test.mdg apache-log4j-test-hillclimbing hillclimbing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = \"java -jar alvin_bunch_automation.jar apache-log4j-test.mdg apache-log4j-test-hillclimbing hillclimbing\"\n",
    "print(command)\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "fk\n"
     ]
    }
   ],
   "source": [
    "fk = [2,5]\n",
    "print(2 in fk)\n",
    "\n",
    "if 2 in fk:\n",
    "    print('fk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = 'raw_depends/' + project_name\n",
    "json_name_dict = {}\n",
    "json_result_dict = {}\n",
    "for root, dirs, files in os.walk(rootdir):\n",
    "   \n",
    "    #print(files)\n",
    "    for element in files:\n",
    "        \n",
    "        with open(rootdir + '/' + element) as f:\n",
    "            tmp = json.load(f)\n",
    "        \n",
    "        element = element.split('_')\n",
    "        json_name_dict[element[0]] = element[1]\n",
    "        json_result_dict[element[0]] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
