{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import jellyfish\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe_from_file(filename, filesubstr):\n",
    "    \n",
    "    final_results = []\n",
    "    initial_line = 2 - 1\n",
    "    initial_line_result = 5 - 1\n",
    "    \n",
    "    num_class_f = open('Final_Results/' + filesubstr + '/num_class.txt')\n",
    "    for line in num_class_f:\n",
    "        num_class = line\n",
    "        \n",
    "    #print(num_class)\n",
    "\n",
    "    f = open(filename, 'r')\n",
    "    tmp_arr = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i == initial_line:\n",
    "            line = line.replace('MoJo', '')\n",
    "            line = line.replace('.rsf', '')\n",
    "            line = line.replace('(', '')\n",
    "            line = line.replace(')', '')\n",
    "            line = line.split('_')\n",
    "            name = line[0].split('/')[0]\n",
    "            version = line[0].split('-')[-1]\n",
    "            n_cluster = line[1]\n",
    "            affinity = line[2]\n",
    "            linkage = line[3]\n",
    "            #print(line)\n",
    "            #print('Version', version)\n",
    "            initial_line += 5\n",
    "            tmp_arr.append(name)\n",
    "            tmp_arr.append(version)\n",
    "            tmp_arr.append(n_cluster)\n",
    "            tmp_arr.append(affinity)\n",
    "            tmp_arr.append(linkage)\n",
    "        elif i == initial_line_result:\n",
    "            n_MoJo = line.split(' ')[-1].split('\\n')[0]\n",
    "            \n",
    "            #print(line)\n",
    "            initial_line_result += 5\n",
    "            tmp_arr.append(n_MoJo)\n",
    "            tmp_arr.append((1 - (float(n_MoJo) / float(num_class))) * 100)\n",
    "            final_results.append(tmp_arr)\n",
    "            tmp_arr = []\n",
    "            \n",
    "    \n",
    "    \n",
    "    f.close()\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-storm\n",
      "\n",
      "apache-cassandra\n",
      "\n",
      "airbnb-lottie-android\n",
      "\n",
      "apache-isis\n",
      "\n",
      "apache-jmeter\n",
      "\n",
      "apache-log4j\n",
      "\n",
      "apache-maven\n",
      "\n",
      "apache-spark\n",
      "\n",
      "apache-tomcat\n",
      "\n",
      "rzwitserloot-lombok\n",
      "\n",
      "apache-tika\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the smallest MoJo Value\n",
    "rootdir = 'Final_Results/'\n",
    "project_names = []\n",
    "\n",
    "f = open('project_list.txt', 'r')\n",
    "for line in f:\n",
    "    print(line)\n",
    "    project_names.append(line.split('\\n')[0])\n",
    "\n",
    "#project_names = ['apache-maven', 'apache-storm']\n",
    "dir_arr = []\n",
    "\n",
    "#Main clustering arr to be transformed to pandas dataframe later\n",
    "cluster_main_arr = []\n",
    "\n",
    "#Main CK arr to be transformed to pandas dataframe later\n",
    "ck_sum_arr = []\n",
    "ck_max_arr = []\n",
    "ck_std_arr = []\n",
    "ck_mean_arr = []\n",
    "\n",
    "# Only considering the below ck_metrics\n",
    "ck_metrics = ['cbo', 'wmc', 'dit', 'rfc', 'lcom',\n",
    "       'totalMethods', 'staticMethods', 'publicMethods', 'privateMethods',\n",
    "       'protectedMethods', 'defaultMethods', 'abstractMethods', 'finalMethods',\n",
    "       'synchronizedMethods', 'totalFields', 'staticFields', 'publicFields',\n",
    "       'privateFields', 'protectedFields', 'defaultFields', 'finalFields',\n",
    "       'synchronizedFields', 'nosi', 'loc', 'returnQty', 'loopQty',\n",
    "       'comparisonsQty', 'tryCatchQty', 'parenthesizedExpsQty',\n",
    "       'stringLiteralsQty', 'numbersQty', 'assignmentsQty',\n",
    "       'mathOperationsQty', 'variablesQty', 'maxNestedBlocks',\n",
    "       'anonymousClassesQty', 'subClassesQty', 'lambdasQty', 'uniqueWordsQty',\n",
    "       'modifiers']\n",
    "\n",
    "depth = 3\n",
    "for project_name in project_names:\n",
    "    rootdir += project_name\n",
    "    #print(rootdir)\n",
    "    for root, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            \n",
    "            dir_arr.append(rootdir + '/' + str(file))\n",
    "            \n",
    "    for element in dir_arr:\n",
    "        ver = str(element).replace('_class.csv', '')\n",
    "        ver = ver.split('-')[-1]\n",
    "        \n",
    "        if '_class.csv' in element:\n",
    "            \n",
    "            df_sum = pd.read_csv(element)\n",
    "            current_element_sum = df_sum.sum()\n",
    "            \n",
    "            df_max = pd.read_csv(element)\n",
    "            current_element_max = df_max.max()\n",
    "            \n",
    "            df_std = pd.read_csv(element)\n",
    "            current_element_std = df_std.std()\n",
    "            \n",
    "            df_mean = pd.read_csv(element)\n",
    "            current_element_mean = df_mean.mean()\n",
    "            \n",
    "            tmp_sum = []\n",
    "            tmp_max = []\n",
    "            tmp_std = []\n",
    "            tmp_mean = []\n",
    "            \n",
    "            tmp_sum.append(project_name)\n",
    "            tmp_sum.append(ver)\n",
    "            \n",
    "            tmp_max.append(project_name)\n",
    "            tmp_max.append(ver)\n",
    "            \n",
    "            tmp_std.append(project_name)\n",
    "            tmp_std.append(ver)\n",
    "            \n",
    "            tmp_mean.append(project_name)\n",
    "            tmp_mean.append(ver)\n",
    "            \n",
    "            for element in ck_metrics:\n",
    "                tmp_sum.append(current_element_sum[element])\n",
    "                tmp_max.append(current_element_max[element])\n",
    "                tmp_std.append(current_element_std[element])\n",
    "                tmp_mean.append(current_element_mean[element])\n",
    "        \n",
    "            ck_sum_arr.append(tmp_sum)\n",
    "            ck_max_arr.append(tmp_max)\n",
    "            ck_std_arr.append(tmp_std)\n",
    "            ck_mean_arr.append(tmp_mean)\n",
    "        \n",
    "            \n",
    "        elif '_results.txt' in element:\n",
    "            #print(project_name, element)\n",
    "            spark_df = pd.DataFrame(read_dataframe_from_file(element, project_name))\n",
    "            spark_df.columns = ['name', 'version', 'n_clusters', 'affinity', 'linkage', 'n_MoJo', 'mojoFM']\n",
    "            #spark_df['mojoFM'] = (1 - spark_df['n_MoJo'].astype(int)/spark_count)*100\n",
    "            #print(spark_df[spark_df.n_MoJo == spark_df.n_MoJo.min()].iloc[0])\n",
    "            #print(spark_df.loc[spark_df.groupby('version').mojoFM.idxmax()].reset_index(drop=True))\n",
    "    for element in spark_df.loc[spark_df.groupby('version').mojoFM.idxmax()].reset_index(drop=True).iterrows():\n",
    "        tmp_arr2 = []\n",
    "        tmp_arr2.append(element[1]['name'])\n",
    "        tmp_arr2.append(element[1]['version'])\n",
    "        tmp_arr2.append(element[1]['n_clusters'])\n",
    "        tmp_arr2.append(element[1]['affinity'])\n",
    "        tmp_arr2.append(element[1]['linkage'])\n",
    "        tmp_arr2.append(element[1]['n_MoJo'])\n",
    "        tmp_arr2.append(element[1]['mojoFM'])\n",
    "        \n",
    "        cluster_main_arr.append(tmp_arr2)\n",
    "    #cluster_main_arr.append(spark_df.loc[spark_df.groupby('version').mojoFM.idxmax()].reset_index(drop=True))\n",
    "            #cluster_main_arr.append(spark_df[spark_df.n_MoJo == spark_df.n_MoJo.min()].iloc[0])\n",
    "    \n",
    "    rootdir = 'Final_Results/'\n",
    "    #dir_arr = []\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clustering_result = pd.DataFrame(cluster_main_arr)\n",
    "final_clustering_result.columns = ['name', 'version', 'n_clusters', 'affinity', 'linkage', 'n_MoJo', 'mojoFM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clustering_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clustering_result['name'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clustering_result.to_csv('clustering_result_10_ver.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For CK Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SUM CK Metric\n",
    "\n",
    "ck_sum_df = pd.DataFrame(ck_sum_arr)\n",
    "final_ck_columns = ['project_name', 'version']\n",
    "for element in ck_metrics:\n",
    "    final_ck_columns.append(element)\n",
    "ck_sum_df.columns = final_ck_columns\n",
    "ck_sum_df.head()\n",
    "\n",
    "ck_sum_df.to_csv('ck_sum_10_ver.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Max CK Metric\n",
    "\n",
    "ck_max_df = pd.DataFrame(ck_max_arr)\n",
    "final_ck_columns = ['project_name', 'version']\n",
    "for element in ck_metrics:\n",
    "    final_ck_columns.append(element)\n",
    "ck_max_df.columns = final_ck_columns\n",
    "ck_max_df.head()\n",
    "\n",
    "ck_max_df.to_csv('ck_max_10_ver.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SUM CK Metric\n",
    "\n",
    "ck_std_arr = pd.DataFrame(ck_std_arr)\n",
    "final_ck_columns = ['project_name', 'version']\n",
    "for element in ck_metrics:\n",
    "    final_ck_columns.append(element)\n",
    "ck_std_arr.columns = final_ck_columns\n",
    "ck_std_arr.head()\n",
    "\n",
    "ck_std_arr.to_csv('ck_std_10_ver.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SUM CK Metric\n",
    "\n",
    "ck_mean_arr = pd.DataFrame(ck_mean_arr)\n",
    "final_ck_columns = ['project_name', 'version']\n",
    "for element in ck_metrics:\n",
    "    final_ck_columns.append(element)\n",
    "ck_mean_arr.columns = final_ck_columns\n",
    "ck_mean_arr.head()\n",
    "\n",
    "ck_mean_arr.to_csv('ck_mean_10_ver.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.project_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
